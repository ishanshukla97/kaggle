{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, roc_auc_score, f1_score\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "from time import time\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(2)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imread\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import h5py\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29220,) (29220, 29) (3108,) (3108, 29)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00008af0-bad0-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000a892-bacf-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0006faa6-bac7-11e8-b2b7-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0008baca-bad7-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000cce7e-bad4-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00109f6a-bac8-11e8-b2b7-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>001765de-bacd-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0018641a-bac9-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00200f22-bad7-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0026f154-bac6-11e8-b2b7-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>002729d2-bace-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>002c1a7c-bad4-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>003170fa-bacd-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0031820a-baca-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00407c16-bad3-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>005ce2ea-bacc-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>00631ec8-bad9-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>00673f64-bad2-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0070171c-bad0-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>007290b6-bad8-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0075ee26-bacb-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>00763d66-bacd-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>007eca68-bac7-11e8-b2b7-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>008ab0b8-bad5-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>008e8c3e-bad4-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>009131e6-baca-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0094159e-bad1-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0097a5c2-bac9-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>009d32e4-bad3-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>00a4925a-bad8-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11672</th>\n",
       "      <td>ff2cd716-bad7-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11673</th>\n",
       "      <td>ff2d3abc-bad7-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11674</th>\n",
       "      <td>ff38cec0-bad4-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11675</th>\n",
       "      <td>ff489096-bad7-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11676</th>\n",
       "      <td>ff49d834-bad2-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11677</th>\n",
       "      <td>ff4aa9a4-bac7-11e8-b2b7-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11678</th>\n",
       "      <td>ff5464dc-baca-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11679</th>\n",
       "      <td>ff56b30a-bace-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11680</th>\n",
       "      <td>ff581dac-bac5-11e8-b2b7-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11681</th>\n",
       "      <td>ff5ee18c-baca-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11682</th>\n",
       "      <td>ff67db02-baca-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11683</th>\n",
       "      <td>ff7cc20c-bad3-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11684</th>\n",
       "      <td>ff7dc452-bad5-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11685</th>\n",
       "      <td>ff8346fa-bad7-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11686</th>\n",
       "      <td>ff8e580c-bad1-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11687</th>\n",
       "      <td>ff9659a0-bacf-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11688</th>\n",
       "      <td>ff9bdfd8-baca-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11689</th>\n",
       "      <td>ff9ccd2a-bad0-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11690</th>\n",
       "      <td>ffa6bc7e-bad3-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11691</th>\n",
       "      <td>ffad96dc-bad5-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11692</th>\n",
       "      <td>ffb91448-bac7-11e8-b2b7-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11693</th>\n",
       "      <td>ffd677a0-bada-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11694</th>\n",
       "      <td>ffd72db2-bad5-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11695</th>\n",
       "      <td>ffd83fa4-bacb-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11696</th>\n",
       "      <td>ffd91122-bad0-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11697</th>\n",
       "      <td>ffdfb96a-bad4-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11698</th>\n",
       "      <td>ffdfc590-bad4-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11699</th>\n",
       "      <td>ffecb8a4-bad4-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11700</th>\n",
       "      <td>fff03816-bad5-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11701</th>\n",
       "      <td>fffe6f9c-bacd-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11702 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Id  Predicted\n",
       "0      00008af0-bad0-11e8-b2b8-ac1f6b6435d0          0\n",
       "1      0000a892-bacf-11e8-b2b8-ac1f6b6435d0          0\n",
       "2      0006faa6-bac7-11e8-b2b7-ac1f6b6435d0          0\n",
       "3      0008baca-bad7-11e8-b2b9-ac1f6b6435d0          0\n",
       "4      000cce7e-bad4-11e8-b2b8-ac1f6b6435d0          0\n",
       "5      00109f6a-bac8-11e8-b2b7-ac1f6b6435d0          0\n",
       "6      001765de-bacd-11e8-b2b8-ac1f6b6435d0          0\n",
       "7      0018641a-bac9-11e8-b2b8-ac1f6b6435d0          0\n",
       "8      00200f22-bad7-11e8-b2b9-ac1f6b6435d0          0\n",
       "9      0026f154-bac6-11e8-b2b7-ac1f6b6435d0          0\n",
       "10     002729d2-bace-11e8-b2b8-ac1f6b6435d0          0\n",
       "11     002c1a7c-bad4-11e8-b2b8-ac1f6b6435d0          0\n",
       "12     003170fa-bacd-11e8-b2b8-ac1f6b6435d0          0\n",
       "13     0031820a-baca-11e8-b2b8-ac1f6b6435d0          0\n",
       "14     00407c16-bad3-11e8-b2b8-ac1f6b6435d0          0\n",
       "15     005ce2ea-bacc-11e8-b2b8-ac1f6b6435d0          0\n",
       "16     00631ec8-bad9-11e8-b2b9-ac1f6b6435d0          0\n",
       "17     00673f64-bad2-11e8-b2b8-ac1f6b6435d0          0\n",
       "18     0070171c-bad0-11e8-b2b8-ac1f6b6435d0          0\n",
       "19     007290b6-bad8-11e8-b2b9-ac1f6b6435d0          0\n",
       "20     0075ee26-bacb-11e8-b2b8-ac1f6b6435d0          0\n",
       "21     00763d66-bacd-11e8-b2b8-ac1f6b6435d0          0\n",
       "22     007eca68-bac7-11e8-b2b7-ac1f6b6435d0          0\n",
       "23     008ab0b8-bad5-11e8-b2b8-ac1f6b6435d0          0\n",
       "24     008e8c3e-bad4-11e8-b2b8-ac1f6b6435d0          0\n",
       "25     009131e6-baca-11e8-b2b8-ac1f6b6435d0          0\n",
       "26     0094159e-bad1-11e8-b2b8-ac1f6b6435d0          0\n",
       "27     0097a5c2-bac9-11e8-b2b8-ac1f6b6435d0          0\n",
       "28     009d32e4-bad3-11e8-b2b8-ac1f6b6435d0          0\n",
       "29     00a4925a-bad8-11e8-b2b9-ac1f6b6435d0          0\n",
       "...                                     ...        ...\n",
       "11672  ff2cd716-bad7-11e8-b2b9-ac1f6b6435d0          0\n",
       "11673  ff2d3abc-bad7-11e8-b2b9-ac1f6b6435d0          0\n",
       "11674  ff38cec0-bad4-11e8-b2b8-ac1f6b6435d0          0\n",
       "11675  ff489096-bad7-11e8-b2b9-ac1f6b6435d0          0\n",
       "11676  ff49d834-bad2-11e8-b2b8-ac1f6b6435d0          0\n",
       "11677  ff4aa9a4-bac7-11e8-b2b7-ac1f6b6435d0          0\n",
       "11678  ff5464dc-baca-11e8-b2b8-ac1f6b6435d0          0\n",
       "11679  ff56b30a-bace-11e8-b2b8-ac1f6b6435d0          0\n",
       "11680  ff581dac-bac5-11e8-b2b7-ac1f6b6435d0          0\n",
       "11681  ff5ee18c-baca-11e8-b2b8-ac1f6b6435d0          0\n",
       "11682  ff67db02-baca-11e8-b2b8-ac1f6b6435d0          0\n",
       "11683  ff7cc20c-bad3-11e8-b2b8-ac1f6b6435d0          0\n",
       "11684  ff7dc452-bad5-11e8-b2b9-ac1f6b6435d0          0\n",
       "11685  ff8346fa-bad7-11e8-b2b9-ac1f6b6435d0          0\n",
       "11686  ff8e580c-bad1-11e8-b2b8-ac1f6b6435d0          0\n",
       "11687  ff9659a0-bacf-11e8-b2b8-ac1f6b6435d0          0\n",
       "11688  ff9bdfd8-baca-11e8-b2b8-ac1f6b6435d0          0\n",
       "11689  ff9ccd2a-bad0-11e8-b2b8-ac1f6b6435d0          0\n",
       "11690  ffa6bc7e-bad3-11e8-b2b8-ac1f6b6435d0          0\n",
       "11691  ffad96dc-bad5-11e8-b2b9-ac1f6b6435d0          0\n",
       "11692  ffb91448-bac7-11e8-b2b7-ac1f6b6435d0          0\n",
       "11693  ffd677a0-bada-11e8-b2b9-ac1f6b6435d0          0\n",
       "11694  ffd72db2-bad5-11e8-b2b9-ac1f6b6435d0          0\n",
       "11695  ffd83fa4-bacb-11e8-b2b8-ac1f6b6435d0          0\n",
       "11696  ffd91122-bad0-11e8-b2b8-ac1f6b6435d0          0\n",
       "11697  ffdfb96a-bad4-11e8-b2b8-ac1f6b6435d0          0\n",
       "11698  ffdfc590-bad4-11e8-b2b8-ac1f6b6435d0          0\n",
       "11699  ffecb8a4-bad4-11e8-b2b8-ac1f6b6435d0          0\n",
       "11700  fff03816-bad5-11e8-b2b9-ac1f6b6435d0          0\n",
       "11701  fffe6f9c-bacd-11e8-b2b8-ac1f6b6435d0          0\n",
       "\n",
       "[11702 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('aug_train.csv')\n",
    "label_names = {\n",
    "    0:  \"Nucleoplasm\",  \n",
    "    1:  \"Nuclear membrane\",   \n",
    "    2:  \"Nucleoli\",   \n",
    "    3:  \"Nucleoli fibrillar center\",   \n",
    "    4:  \"Nuclear speckles\",\n",
    "    5:  \"Nuclear bodies\",   \n",
    "    6:  \"Endoplasmic reticulum\",   \n",
    "    7:  \"Golgi apparatus\",   \n",
    "    8:  \"Peroxisomes\",   \n",
    "    9:  \"Endosomes\",   \n",
    "    10:  \"Lysosomes\",   \n",
    "    11:  \"Intermediate filaments\",   \n",
    "    12:  \"Actin filaments\",   \n",
    "    13:  \"Focal adhesion sites\",   \n",
    "    14:  \"Microtubules\",   \n",
    "    15:  \"Microtubule ends\",   \n",
    "    16:  \"Cytokinetic bridge\",   \n",
    "    17:  \"Mitotic spindle\",   \n",
    "    18:  \"Microtubule organizing center\",   \n",
    "    19:  \"Centrosome\",   \n",
    "    20:  \"Lipid droplets\",   \n",
    "    21:  \"Plasma membrane\",   \n",
    "    22:  \"Cell junctions\",   \n",
    "    23:  \"Mitochondria\",   \n",
    "    24:  \"Aggresome\",   \n",
    "    25:  \"Cytosol\",   \n",
    "    26:  \"Cytoplasmic bodies\",   \n",
    "    27:  \"Rods & rings\"\n",
    "}\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "odf = pd.read_csv('train.csv')\n",
    "for k in label_names.keys():\n",
    "    odf[label_names[k]] = 0\n",
    "\n",
    "def one_hot(row):\n",
    "    row.Target = np.array(row.Target.split(\" \")).astype(np.int)\n",
    "    \n",
    "    for target in row.Target:\n",
    "        row.loc[label_names[int(target)]] = 1\n",
    "    return row\n",
    "odf = odf.apply(one_hot, axis=1)\n",
    "odf = odf.drop('Target', axis=1)\n",
    "\n",
    "def class_weights(df):\n",
    "    total = 0\n",
    "    weights = dict()\n",
    "    for idx,key in label_names.items():\n",
    "        total += df[key].value_counts()[1]\n",
    "    \n",
    "    for idx,key in label_names.items():\n",
    "        weights[idx] = total/df[key].value_counts()[1]\n",
    "    \n",
    "    return weights\n",
    "\n",
    "aug1 = pd.read_csv('aug_train1.csv')\n",
    "aug1 = aug1.drop('Unnamed: 0', axis=1)\n",
    "_, X_test, _, y_test = train_test_split(odf.Id, odf, test_size=0.1, shuffle=True, random_state=2)\n",
    "X_train, y_train = aug1.Id, aug1\n",
    "\n",
    "X_train.reset_index(inplace=True, drop=True)\n",
    "y_train.reset_index(inplace=True, drop=True)\n",
    "X_test.reset_index(inplace=True, drop=True)\n",
    "y_test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "weights = class_weights(y_train)\n",
    "\n",
    "sub_csv = pd.read_csv('test.csv')\n",
    "sub_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f1 accuracy and focal loss function\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "\n",
    "def fl(y_true, y_pred):#with tensorflow\n",
    "    gamma=2\n",
    "    alpha=1\n",
    "    eps = 1e-16\n",
    "    y_pred=K.clip(y_pred,eps,1-eps)#improve the stability of the focal loss and see issues 1 for more information\n",
    "    pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "    pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "    pt_1 = K.clip(pt_1, 1e-6, 1.-1e-6)\n",
    "    pt_0 = K.clip(pt_0, 1e-6, 1.-1e-6)\n",
    "\n",
    "    return -(K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) + \n",
    "            K.sum(alpha * K.pow(1-(1-pt_0), gamma) * K.log(1. - pt_0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelParameters():\n",
    "    def __init__(self):\n",
    "        self.n_classes = 28\n",
    "        self.image_rows = 512\n",
    "        self.image_cols = 512\n",
    "        self.n_channels = 4\n",
    "        self.batch_size = 16\n",
    "        self.n_epochs = 50\n",
    "        self.shuffle = False\n",
    "        self.image_row_scale_factor = 1\n",
    "        self.image_col_scale_factor = 1\n",
    "        self.scaled_row_dim = np.int(self.image_rows/self.image_row_scale_factor)\n",
    "        self.scaled_col_dim = np.int(self.image_cols/self.image_row_scale_factor)\n",
    "        self.input_shape = (self.scaled_row_dim, self.scaled_col_dim, self.n_channels)\n",
    "\n",
    "params = ModelParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, X_train, y_train, X_test, y_test, params, X_sub=None):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.params = params\n",
    "        self.X_sub = X_sub\n",
    "        \n",
    "    def build_iterators(self, num_threads=6, num_prefetch=8):\n",
    "        def parse_function(filename, label):\n",
    "            dims = (params.scaled_row_dim, params.scaled_col_dim)\n",
    "            c_image = [None] * 4\n",
    "\n",
    "            for idx,color in enumerate(['_green', '_red', '_blue', '_yellow']):\n",
    "                fullname = 'test/' + filename + color + '.png'\n",
    "                image_string = tf.read_file(fullname)\n",
    "                c_image[idx] = tf.image.decode_png(image_string, channels=1)\n",
    "                c_image[idx] = tf.image.convert_image_dtype(c_image[idx], tf.float32)\n",
    "                c_image[idx] = tf.image.resize_images(c_image[idx], dims)\n",
    "                c_image[idx] = tf.squeeze(c_image[idx], axis=-1)\n",
    "\n",
    "            image = tf.stack([c_image[0],\n",
    "                              c_image[1],\n",
    "                              c_image[2],\n",
    "                              c_image[3]], axis=-1)\n",
    "            return image, label\n",
    "\n",
    "        \n",
    "        #Build graph - train_dataset\n",
    "        filenames = self.X_train.as_matrix()\n",
    "        labels = self.y_train.drop(['Id'], axis=1).as_matrix()\n",
    "        self.set_name = 'train/'\n",
    "        \n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "        train_dataset = train_dataset.shuffle(len(filenames))\n",
    "        train_dataset = train_dataset.map(parse_function, num_parallel_calls=num_threads)\n",
    "        train_dataset = train_dataset.batch(self.params.batch_size)\n",
    "        train_dataset = train_dataset.prefetch(num_prefetch)\n",
    "        \n",
    "        #Build graph - valid_dataset\n",
    "        filenames = self.X_test.as_matrix()\n",
    "        labels = self.y_test.drop(['Id'], axis=1).as_matrix()\n",
    "        self.set_name = 'test/'\n",
    "        \n",
    "        test_dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "        test_dataset = test_dataset.shuffle(len(filenames))\n",
    "        test_dataset = test_dataset.map(parse_function, num_parallel_calls=num_threads)\n",
    "        test_dataset = test_dataset.batch(self.params.batch_size)\n",
    "        test_dataset = test_dataset.prefetch(num_prefetch)\n",
    "        \n",
    "        #Build graph - test submission\n",
    "        filenames = self.X_sub.as_matrix()\n",
    "        labels = np.zeros(shape=(len(filenames), 28), dtype=np.int64)\n",
    "        sub_dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "        sub_dataset = sub_dataset.map(parse_function, num_parallel_calls=num_threads).batch(self.params.batch_size).prefetch(num_prefetch)\n",
    "        \n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                                   train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "        \n",
    "        self.train_init_op = iterator.make_initializer(train_dataset)\n",
    "        self.test_init_op = iterator.make_initializer(test_dataset)\n",
    "        self.sub_init_op = iterator.make_initializer(sub_dataset)\n",
    "        return next_element\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.densenet_121 import get_model\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        \n",
    "\n",
    "    def build(self, lr=0.01):\n",
    "        self.data_loader = DataLoader(X_train, y_train, X_test, y_test, self.params, X_sub=sub_csv.Id)\n",
    "        next_element = self.data_loader.build_iterators()\n",
    "\n",
    "        #tf.reset_default_graph()\n",
    "\n",
    "        with tf.name_scope('placeholders'):\n",
    "\n",
    "            #self.y = tf.placeholder(tf.float32, shape=[None, params.n_classes], name='y')\n",
    "            self.y = next_element[1]\n",
    "            \n",
    "        with tf.name_scope('convolutional'):\n",
    "            out = get_model(next_element[0], 28, 'channels_last', True)\n",
    "\n",
    "        with tf.name_scope('dense'):\n",
    "            dense = tf.layers.dense(out, 64, activation=tf.nn.relu, name='dense')\n",
    "            self.output = tf.layers.dense(dense, 28, activation=tf.nn.sigmoid, name='output')\n",
    "\n",
    "        with tf.name_scope('loss'):\n",
    "            self.entropy = fl(self.y, self.output)\n",
    "            self.loss = tf.reduce_mean(self.entropy)\n",
    "\n",
    "        with tf.name_scope('train'):\n",
    "            self.optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "            self.train_op = self.optimizer.minimize(self.loss)\n",
    "            \n",
    "    def learn(self, epochs=150):\n",
    "        saver = tf.train.Saver()\n",
    "        with tf.Session() as sess:\n",
    "            \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            saver.restore(sess, 'dense_models/densenet_stage2_4.ckpt')\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                self.score(session=sess)\n",
    "                train_len = int(np.floor(len(X_train)/self.params.batch_size))\n",
    "                loss_all = []\n",
    "                \n",
    "                print('Epoch: ' + str(epoch))\n",
    "                with tqdm(total=train_len) as pbar:\n",
    "                    sess.run(self.data_loader.train_init_op)\n",
    "                    \n",
    "                    for steps in range(train_len):\n",
    "                        train, loss = sess.run([self.train_op, self.loss])\n",
    "                        loss_all.append(loss)\n",
    "                        pbar.set_description('loss: {:.4f}'.format(loss))\n",
    "                        pbar.update(1)\n",
    "                    \n",
    "                    print('train_loss: ' + str(np.mean(loss_all)))\n",
    "                saver.save(sess, 'dense_models/densenet_stage2_5.ckpt')\n",
    "    \n",
    "    def score(self, session=None, gen='test'):\n",
    "        with session.as_default():\n",
    "            if gen == 'test':\n",
    "                gen_len = int(np.floor(len(X_test)/self.params.batch_size))\n",
    "                \n",
    "                preds_all = np.zeros((gen_len * self.params.batch_size, 28))\n",
    "                true = np.zeros((gen_len * self.params.batch_size, 28), dtype=np.int8)\n",
    "                loss_all = []\n",
    "                \n",
    "                session.run(self.data_loader.test_init_op)\n",
    "                with tqdm(total=gen_len) as pbar:\n",
    "                    for steps in range(gen_len):\n",
    "\n",
    "                        preds, loss, y_batch = session.run([self.output, self.loss, self.y])\n",
    "                        \n",
    "                        true[steps:steps+self.params.batch_size, :] = y_batch\n",
    "                        preds_all[steps:steps+self.params.batch_size, :] = preds\n",
    "                        loss_all.append(loss)\n",
    "                        \n",
    "                        pbar.set_description('loss: {:.4f}'.format(loss))\n",
    "                        pbar.update(1)\n",
    "                        \n",
    "\n",
    "                    preds_arr = preds_all > 0.5\n",
    "\n",
    "                    assert(preds_arr.shape == true.shape, 'Pred, truth shape mismatch')\n",
    "                    f1 = f1_score(true, preds_arr, average='macro')\n",
    "                    precision = precision_score(true, preds_arr, average='macro')\n",
    "                    print('F1: ' + str(f1), 'Precision: ' + str(precision), 'val_loss: ' + str(np.mean(loss_all)))\n",
    "                    return preds_arr\n",
    "                \n",
    "    def create_submission(self, session=None, X=None):\n",
    "        saver = tf.train.Saver()\n",
    "        p_all = []\n",
    "        with tf.Session() as session:\n",
    "            gen_len = int(np.floor(len(X)/self.params.batch_size))\n",
    "            session.run(tf.global_variables_initializer())\n",
    "            \n",
    "            preds_all = np.zeros((gen_len * self.params.batch_size, 28))\n",
    "            \n",
    "            saver.restore(session, 'dense_models/densenet_stage2_5.ckpt')\n",
    "            \n",
    "            session.run(self.data_loader.sub_init_op)\n",
    "            with tqdm(total=gen_len) as pbar:\n",
    "                for steps in range(gen_len):\n",
    "                    preds = session.run([self.output])\n",
    "                    preds_all[steps:steps+self.params.batch_size] = preds[0]\n",
    "                    p_all.append(preds[0])\n",
    "                    pbar.update(1)\n",
    "                    \n",
    "                preds_ar = np.array(p_all)\n",
    "\n",
    "                preds_ar = np.reshape(preds_ar, (preds_ar.shape[0]*preds_ar.shape[1],preds_ar.shape[2]))\n",
    "\n",
    "                preds_arr0 = preds_all > 0.5\n",
    "                \n",
    "                return preds_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Model(params)\n",
    "model.build(lr=1e-5)\n",
    "#model.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from dense_models/densenet_stage2_5.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 731/731 [01:38<00:00,  9.40it/s]\n"
     ]
    }
   ],
   "source": [
    "subm = model.create_submission(X=sub_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_preds = []\n",
    "threshold = 0.1\n",
    "sub = subm > threshold\n",
    "for line in sub:\n",
    "    i=0\n",
    "    str_label = ''\n",
    "    flag=False\n",
    "    for c_pred in line:\n",
    "        if c_pred:\n",
    "            str_label += str(i) + ' '\n",
    "            flag=True\n",
    "        else:\n",
    "            str_label += ''\n",
    "        i += 1\n",
    "        \n",
    "    list_preds.append(str_label.strip())\n",
    "\n",
    "    \n",
    "for i in range(6):\n",
    "    list_preds.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "submissions = pd.DataFrame({'Id': sub_csv.Id, 'Predicted': list_preds})\n",
    "\n",
    "submissions.to_csv('submission.csv', index=False)\n",
    "submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dense_models/densenet_stage2_1.ckpt -- Epochs: 14 -- lr: 1e-3\n",
    "                                        loss: 15.1176: 100%|██████████| 1826/1826 [11:47<00:00,  2.77it/s]\n",
    "                                        train_loss: 11.212003\n",
    "                                        loss: 11.1562: 100%|██████████| 194/194 [00:23<00:00,  8.81it/s]\n",
    "                                          0%|          | 0/1826 [00:00<?, ?it/s]\n",
    "                                        F1: 0.41749941032695653 Precision: 0.5143140034299384 val_loss: 11.958441\n",
    "                                        \n",
    "### dense_models/densenet_stage2_2.ckpt -- Epochs: 4 -- lr:1e-4\n",
    "                                        loss: 12.0911: 100%|██████████| 1826/1826 [11:49<00:00,  2.79it/s]\n",
    "                                        train_loss: 9.5457945\n",
    "                                        loss: 9.6333: 100%|██████████| 194/194 [00:23<00:00,  8.63it/s] \n",
    "                                          0%|          | 0/1826 [00:00<?, ?it/s]\n",
    "                                        F1: 0.5170505314840885 Precision: 0.6042577958457712 val_loss: 10.7184\n",
    "                                        \n",
    "### dense_models/densenet_stage2_3.ckpt -- Epoch: 2 -- lr:1e-5\n",
    "                                        loss: 11.7702: 100%|██████████| 1826/1826 [11:49<00:00,  2.81it/s]\n",
    "                                        train_loss: 9.353686\n",
    "                                        loss: 9.6262: 100%|██████████| 194/194 [00:23<00:00,  8.18it/s] \n",
    "                                          0%|          | 0/1826 [00:00<?, ?it/s]\n",
    "                                        F1: 0.534295450029398 Precision: 0.6561235025549912 val_loss: 10.566189"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
